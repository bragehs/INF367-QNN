{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MANDATORY ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1) data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 7.9\n",
      "0 2\n"
     ]
    }
   ],
   "source": [
    "print(np.min(X), np.max(X))\n",
    "print(np.min(Y), np.max(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from qiskit import QuantumCircuit, transpile, assemble\n",
    "from qiskit_aer import Aer, AerSimulator\n",
    "from qiskit.visualization import plot_histogram\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit_algorithms.optimizers import SPSA\n",
    "\n",
    "from sklearn.metrics import log_loss # loss function\n",
    "from sklearn.metrics import accuracy_score # accuracy\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, np.pi)) # since we are using angle encoding, we need to scale from 0 to pi\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumMachineLearning:\n",
    "    def __init__(self, X_train, y_train, X_val, y_val, num_qubits = 4, num_layers = 3):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.num_qubits = num_qubits\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = len(set(y_train))\n",
    "        self.rng = np.random.default_rng(42)\n",
    "        self.initial_parameters = self.rng.uniform(0, np.pi, self.num_qubits * self.num_layers)\n",
    "\n",
    "        self.base_circuit = self._create_base_circuit()\n",
    "        self.backend = AerSimulator(method = 'statevector')\n",
    "    \n",
    "    def angle_encoding(self, qc, data_point):\n",
    "        [qc.rx(data_point[qubit], qubit) for qubit in range(self.num_qubits)]\n",
    "        return qc\n",
    "\n",
    "    def real_amplitudes(self, qc, parameters):\n",
    "        param_indices = np.arange(len(parameters)).reshape(self.num_layers, self.num_qubits)\n",
    "        \n",
    "        for layer in range(self.num_layers):\n",
    "            [qc.ry(parameters[param_indices[layer, qubit]], qubit) \n",
    "             for qubit in range(self.num_qubits)]\n",
    "            qc.barrier()\n",
    "            \n",
    "            [qc.cx(qubit, qubit+1) for qubit in range(self.num_qubits-1)]\n",
    "            qc.barrier()\n",
    "\n",
    "        return qc\n",
    "\n",
    "    def _create_base_circuit(self, encoding = 'angle', circuit = 'real_amplitudes'):\n",
    "        qc = QuantumCircuit(self.num_qubits)\n",
    "        if encoding == 'angle':\n",
    "            qc = self.angle_encoding(qc, np.array([Parameter(f\"theta_{i}\") for i in range(self.num_qubits)]))\n",
    "        \n",
    "        if circuit == 'real_amplitudes':\n",
    "            qc = self.real_amplitudes(qc ,np.array([Parameter(f\"theta_{i}{j}\")\n",
    "                                                     for i in range(self.num_qubits)\n",
    "                                                     for j in range(self.num_layers)]))\n",
    "        \n",
    "        return qc\n",
    "\n",
    "\n",
    "    def prepare_circuit(self, data_point, params):\n",
    "        qc = self.base_circuit.copy()\n",
    "        qc.assign_parameters(np.concatenate([data_point, params]), inplace = True)\n",
    "        \n",
    "        return qc\n",
    "   \n",
    "    def run_circuit(self, data_point, params, shots = 100):\n",
    "        qc = self.prepare_circuit(data_point, params)     \n",
    "        \n",
    "        qc.measure_all()\n",
    "\n",
    "        tqc = transpile(qc, self.backend)\n",
    "\n",
    "        job = self.backend.run(tqc, shots=shots)\n",
    "        result = job.result()\n",
    "        counts = result.get_counts(qc)\n",
    "\n",
    "        return counts\n",
    "\n",
    "    \n",
    "    def data_decoding(self, output):\n",
    "        return int(output, 2) % self.num_classes\n",
    "    \n",
    "    def loss_function(self, updated_params, val = False, shots = 100):\n",
    "        X = self.X_train if not val else self.X_val\n",
    "        y = self.y_train if not val else self.y_val\n",
    "\n",
    "        def process_counts(x):\n",
    "            counts = self.run_circuit(x, updated_params, shots = shots)\n",
    "\n",
    "            count_classes = {x : 0 for x in range(self.num_classes)}\n",
    "            [count_classes.update({self.data_decoding(output): \n",
    "                                 count_classes.get(self.data_decoding(output), 0) + count/shots}) \n",
    "             for output, count in counts.items()]\n",
    "        \n",
    "            return [count_classes[x] for x in range(self.num_classes)]        \n",
    "            \n",
    "        predicted_probabilites = np.array([process_counts(x) for x in X])\n",
    "        logloss = log_loss(y, predicted_probabilites)\n",
    "\n",
    "        #print(f\"Parameters: {updated_params} loss: {logloss}\")\n",
    "        return logloss\n",
    "\n",
    "    def SPSA_optimize(self, maxiter = 50):\n",
    "        optimizer = SPSA(maxiter=maxiter)\n",
    "        # Optimize the parameters\n",
    "        optimized = optimizer.minimize(fun=self.loss_function, x0=self.initial_parameters)\n",
    "\n",
    "        print(\"Optimized Parameters:\", optimized.x)\n",
    "        print(\"Minimum Loss:\", optimized.fun)\n",
    "        self.optimized_params = optimized.x\n",
    "        self.min_loss = optimized.fun\n",
    "    \n",
    "\n",
    "    def gradient(self, params, epsilon = 0.2):\n",
    "        size = len(params)\n",
    "        plus = np.broadcast_to(params, (size, size)) + np.eye(size) * epsilon\n",
    "        minus = np.broadcast_to(params, (size, size)) - np.eye(size) * epsilon\n",
    "        L_plus = np.array([self.loss_function(p) for p in plus])\n",
    "        L_minus = np.array([self.loss_function(p) for p in minus])\n",
    "\n",
    "        return (L_plus - L_minus) / (2 * epsilon)\n",
    "    \n",
    "    \n",
    "    def run_gradient_descent(self, learning_rate = 0.1, maxiter = 50, shots = 100):\n",
    "        self.all_epochs = np.empty((0, 3))\n",
    "        current_point = self.initial_parameters\n",
    "        parameter_storage = np.empty((0, len(current_point)))\n",
    "        early_stopping = False\n",
    "        for epoch in range(maxiter):\n",
    "            start = time.time()\n",
    "\n",
    "            gradients = self.gradient(current_point)\n",
    "            current_point = [current_point[j] - learning_rate * gradients[j] for j in range(len(gradients))]\n",
    "\n",
    "            training_loss  = self.loss_function(current_point, shots= shots)\n",
    "            validation_loss = self.loss_function(current_point, val = True, shots= shots)\n",
    "            parameter_storage = np.vstack([parameter_storage, current_point])\n",
    "\n",
    "            end = time.time()\n",
    "            elapsed_time = end - start\n",
    "            self.all_epochs = np.vstack([self.all_epochs, [training_loss, validation_loss, elapsed_time]])\n",
    "\n",
    "            if epoch >= 10:\n",
    "                val_losses = self.all_epochs[:,1]\n",
    "                if np.mean(val_losses[-5:]) > np.mean(val_losses[-10:]): # if last 5 validation losses are greater than last 10, break\n",
    "                    best_index = np.argmin(val_losses)\n",
    "                    current_point = parameter_storage[best_index]\n",
    "                    print(f\"\"\"Early Stopping at epoch {epoch}, \n",
    "                        Training Loss: {self.all_epochs[:,0][best_index]}, \n",
    "                        Validation Loss: {val_losses[best_index]},\"\"\")\n",
    "                    early_stopping = True\n",
    "                    break\n",
    "\n",
    "            print(f\"Epoch {epoch} Training Loss: {training_loss}, Validation Loss: {validation_loss}, Time: {elapsed_time}\")\n",
    "       \n",
    "        current_point = [float(p) for p in current_point]\n",
    "        loss = training_loss if not early_stopping else self.all_epochs[:,0][best_index]\n",
    "        print(\"Optimized Parameters:\", current_point, \"Loss:\", loss)\n",
    "        self.optimized_params = current_point  \n",
    "        self.min_loss = loss\n",
    "    \n",
    "    def predict(self, data_point): #paramteres must be optimized before prediction\n",
    "        prediction_shots = 100000 # more shots as the circuit is only run once\n",
    "        counts = self.run_circuit(data_point, self.optimized_params, shots = prediction_shots)\n",
    "\n",
    "        predicted_probabilites = {x : 0 for x in range(self.num_classes)}\n",
    "            \n",
    "        # Decode each measurement outcome and aggregate probabilites for each class\n",
    "        for output, count in counts.items():\n",
    "            class_num = self.data_decoding(output)\n",
    "            predicted_probabilites[class_num] += count / prediction_shots\n",
    "        \n",
    "        \n",
    "        # Determine the predicted class by choosing the class with the highest probability\n",
    "        predicted_class = max(predicted_probabilites, key=predicted_probabilites.get)\n",
    "        \n",
    "        return predicted_class\n",
    "\n",
    "    def predict_dataset(self, X):\n",
    "        return [self.predict(data_point) for data_point in X]\n",
    "    \n",
    "    def performance(self, y_test, X_test):\n",
    "        self.accuracy = accuracy_score(y_test, self.predict_dataset(X_test))\n",
    "        return self.accuracy\n",
    "\n",
    "    def save(self):\n",
    "        with open('model_data.pkl', 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'optimized_params': self.optimized_params,\n",
    "                'min_loss': self.min_loss,\n",
    "                'accuracy': self.accuracy,\n",
    "                'all_epochs': self.all_epochs\n",
    "            }, f)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.3, random_state=42) # 70% training \n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) # 15% validation, 15% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss: 0.8501950318097843, Validation Loss: 0.641097166047387, Time: 88.16916513442993\n",
      "Epoch 1 Training Loss: 0.7398126483387472, Validation Loss: 0.5259067556102622, Time: 94.84738826751709\n",
      "Epoch 2 Training Loss: 0.6781531016125129, Validation Loss: 0.5211175717499514, Time: 97.23017311096191\n",
      "Epoch 3 Training Loss: 0.6115834101843195, Validation Loss: 0.4284135345393627, Time: 105.76415014266968\n",
      "Epoch 4 Training Loss: 0.5953945315447589, Validation Loss: 0.43530839982679925, Time: 112.65723872184753\n",
      "Epoch 5 Training Loss: 0.5428719825703259, Validation Loss: 0.39427647939522414, Time: 105.55296492576599\n",
      "Epoch 6 Training Loss: 0.5466089500165255, Validation Loss: 0.39502905414139433, Time: 106.61407399177551\n",
      "Epoch 7 Training Loss: 0.5392090471497898, Validation Loss: 0.4132839440290765, Time: 113.57430720329285\n",
      "Epoch 8 Training Loss: 0.5260870927056398, Validation Loss: 0.40624139447883284, Time: 116.4483380317688\n",
      "Epoch 9 Training Loss: 0.5392109552286685, Validation Loss: 0.39254643409824563, Time: 118.03152108192444\n",
      "Epoch 10 Training Loss: 0.5249928866440958, Validation Loss: 0.35622817474639407, Time: 118.19722080230713\n",
      "Epoch 11 Training Loss: 0.5124954234242304, Validation Loss: 0.3744311003353289, Time: 278.3304650783539\n",
      "Epoch 12 Training Loss: 0.5138408349959238, Validation Loss: 0.353507367511988, Time: 218.2242670059204\n",
      "Epoch 13 Training Loss: 0.5065229324511814, Validation Loss: 0.35035016363543037, Time: 95.75329375267029\n",
      "Epoch 14 Training Loss: 0.5072783168052505, Validation Loss: 0.3739770618713926, Time: 103.00503778457642\n",
      "Epoch 15 Training Loss: 0.5024082576853571, Validation Loss: 0.3490474332475705, Time: 122.04165363311768\n",
      "Epoch 16 Training Loss: 0.511544385686994, Validation Loss: 0.34775937590619777, Time: 119.39934992790222\n",
      "Epoch 17 Training Loss: 0.4774281759386893, Validation Loss: 0.3527201606615083, Time: 425.42130613327026\n",
      "Epoch 18 Training Loss: 0.4913856041657475, Validation Loss: 0.3274227222442252, Time: 89.73608589172363\n",
      "Epoch 19 Training Loss: 0.5040579797952657, Validation Loss: 0.3268060650001084, Time: 95.3198173046112\n",
      "Epoch 20 Training Loss: 0.4864435018722168, Validation Loss: 0.3473199855475363, Time: 107.3568787574768\n",
      "Epoch 21 Training Loss: 0.49464836288667524, Validation Loss: 0.33652410590762466, Time: 122.4153790473938\n",
      "Epoch 22 Training Loss: 0.48499381581773987, Validation Loss: 0.33250968163308076, Time: 119.90070033073425\n",
      "Epoch 23 Training Loss: 0.46915203734558414, Validation Loss: 0.34821797502443314, Time: 116.83917379379272\n",
      "Epoch 24 Training Loss: 0.47272657012288033, Validation Loss: 0.3182310323896746, Time: 105.89516019821167\n",
      "Epoch 25 Training Loss: 0.47958419205329006, Validation Loss: 0.337926771232118, Time: 112.2680356502533\n",
      "Epoch 26 Training Loss: 0.4848843376586805, Validation Loss: 0.3205246813711172, Time: 115.49419903755188\n",
      "Epoch 27 Training Loss: 0.46259589626804326, Validation Loss: 0.34124623387368946, Time: 116.67465019226074\n",
      "Epoch 28 Training Loss: 0.47374398615227725, Validation Loss: 0.34739448902962244, Time: 371.5678162574768\n",
      "Epoch 29 Training Loss: 0.4793358471220669, Validation Loss: 0.30949557792815247, Time: 89.88939499855042\n",
      "Epoch 30 Training Loss: 0.46087978384832656, Validation Loss: 0.3428382692672003, Time: 95.6663269996643\n",
      "Epoch 31 Training Loss: 0.4500287954854215, Validation Loss: 0.3051820564549824, Time: 101.82379293441772\n",
      "Epoch 32 Training Loss: 0.4542540573763367, Validation Loss: 0.3466112945388509, Time: 120.36040019989014\n",
      "Epoch 33 Training Loss: 0.4485000417896997, Validation Loss: 0.3259195134486514, Time: 119.94717407226562\n",
      "Early Stopping at epoch 34, \n",
      "                        Training Loss: 0.4500287954854215, \n",
      "                        Validation Loss: 0.3051820564549824,\n",
      "Optimized Parameters: [2.3004669842083616, 0.23847854375513025, 3.1454281409730793, 1.6169980091053295, -0.013246819270140611, 3.522625181927975, 2.080617176657913, 2.848168320103692, -0.008944667770985293, 1.2734739436003266, 1.7404200996009178, 3.422557890377305, 1.8572519924103577, 2.4890481970960607, 1.1649847177572783, 1.725377925129511, 1.4825428589302658, 0.44015431003665956, 2.657233455778988, 1.9471040484051299] Loss: 0.4500287954854215\n"
     ]
    }
   ],
   "source": [
    "object1 = QuantumMachineLearning(X_train, y_train, X_val, y_val, num_layers=5)\n",
    "\n",
    "object1.run_gradient_descent(maxiter = 100, learning_rate=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object1.performance(y_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "object1.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimized_params': [2.3004669842083616,\n",
       "  0.23847854375513025,\n",
       "  3.1454281409730793,\n",
       "  1.6169980091053295,\n",
       "  -0.013246819270140611,\n",
       "  3.522625181927975,\n",
       "  2.080617176657913,\n",
       "  2.848168320103692,\n",
       "  -0.008944667770985293,\n",
       "  1.2734739436003266,\n",
       "  1.7404200996009178,\n",
       "  3.422557890377305,\n",
       "  1.8572519924103577,\n",
       "  2.4890481970960607,\n",
       "  1.1649847177572783,\n",
       "  1.725377925129511,\n",
       "  1.4825428589302658,\n",
       "  0.44015431003665956,\n",
       "  2.657233455778988,\n",
       "  1.9471040484051299],\n",
       " 'min_loss': np.float64(0.4500287954854215),\n",
       " 'accuracy': 1.0,\n",
       " 'all_epochs': array([[8.50195032e-01, 6.41097166e-01, 8.81691651e+01],\n",
       "        [7.39812648e-01, 5.25906756e-01, 9.48473883e+01],\n",
       "        [6.78153102e-01, 5.21117572e-01, 9.72301731e+01],\n",
       "        [6.11583410e-01, 4.28413535e-01, 1.05764150e+02],\n",
       "        [5.95394532e-01, 4.35308400e-01, 1.12657239e+02],\n",
       "        [5.42871983e-01, 3.94276479e-01, 1.05552965e+02],\n",
       "        [5.46608950e-01, 3.95029054e-01, 1.06614074e+02],\n",
       "        [5.39209047e-01, 4.13283944e-01, 1.13574307e+02],\n",
       "        [5.26087093e-01, 4.06241394e-01, 1.16448338e+02],\n",
       "        [5.39210955e-01, 3.92546434e-01, 1.18031521e+02],\n",
       "        [5.24992887e-01, 3.56228175e-01, 1.18197221e+02],\n",
       "        [5.12495423e-01, 3.74431100e-01, 2.78330465e+02],\n",
       "        [5.13840835e-01, 3.53507368e-01, 2.18224267e+02],\n",
       "        [5.06522932e-01, 3.50350164e-01, 9.57532938e+01],\n",
       "        [5.07278317e-01, 3.73977062e-01, 1.03005038e+02],\n",
       "        [5.02408258e-01, 3.49047433e-01, 1.22041654e+02],\n",
       "        [5.11544386e-01, 3.47759376e-01, 1.19399350e+02],\n",
       "        [4.77428176e-01, 3.52720161e-01, 4.25421306e+02],\n",
       "        [4.91385604e-01, 3.27422722e-01, 8.97360859e+01],\n",
       "        [5.04057980e-01, 3.26806065e-01, 9.53198173e+01],\n",
       "        [4.86443502e-01, 3.47319986e-01, 1.07356879e+02],\n",
       "        [4.94648363e-01, 3.36524106e-01, 1.22415379e+02],\n",
       "        [4.84993816e-01, 3.32509682e-01, 1.19900700e+02],\n",
       "        [4.69152037e-01, 3.48217975e-01, 1.16839174e+02],\n",
       "        [4.72726570e-01, 3.18231032e-01, 1.05895160e+02],\n",
       "        [4.79584192e-01, 3.37926771e-01, 1.12268036e+02],\n",
       "        [4.84884338e-01, 3.20524681e-01, 1.15494199e+02],\n",
       "        [4.62595896e-01, 3.41246234e-01, 1.16674650e+02],\n",
       "        [4.73743986e-01, 3.47394489e-01, 3.71567816e+02],\n",
       "        [4.79335847e-01, 3.09495578e-01, 8.98893950e+01],\n",
       "        [4.60879784e-01, 3.42838269e-01, 9.56663270e+01],\n",
       "        [4.50028795e-01, 3.05182056e-01, 1.01823793e+02],\n",
       "        [4.54254057e-01, 3.46611295e-01, 1.20360400e+02],\n",
       "        [4.48500042e-01, 3.25919513e-01, 1.19947174e+02],\n",
       "        [4.67537254e-01, 3.47666822e-01, 1.19559574e+02]])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open ('model_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
