{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MANDATORY ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1) data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 7.9\n",
      "0 2\n"
     ]
    }
   ],
   "source": [
    "print(np.min(X), np.max(X))\n",
    "print(np.min(Y), np.max(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from qiskit import QuantumCircuit, transpile, assemble\n",
    "from qiskit_aer import Aer, AerSimulator\n",
    "from qiskit.visualization import plot_histogram\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit_algorithms.optimizers import SPSA\n",
    "import random\n",
    "from sklearn.metrics import log_loss # loss function\n",
    "from sklearn.metrics import accuracy_score # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, np.pi)) # since we are using angle encoding, we need to scale from 0 to pi\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumMachineLearning:\n",
    "    def __init__(self, X_train, y_train, X_val, y_val, num_qubits = 4, num_layers = 3):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.num_qubits = num_qubits\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = len(set(y_train))\n",
    "        self.rng = np.random.default_rng(42)\n",
    "        self.initial_parameters = self.rng.uniform(0, np.pi, self.num_qubits * self.num_layers)\n",
    "    \n",
    "    def angle_encoding(self, qc, sample):\n",
    "        for qubit in range(len(qc.qubits)):\n",
    "            qc.rx(sample[qubit], qubit)\n",
    "    \n",
    "\n",
    "    def real_amplitudes(self, data_point, parameters):\n",
    "        qc = QuantumCircuit(self.num_qubits)\n",
    "        self.angle_encoding(qc, data_point)\n",
    "\n",
    "        param_index = 0\n",
    "\n",
    "        for layer in range(self.num_layers):\n",
    "            for qubit in range(len(qc.qubits)):\n",
    "                qc.ry(parameters[param_index], qubit)\n",
    "                param_index += 1\n",
    "            qc.barrier()\n",
    "            \n",
    "            for qubit in range(len(qc.qubits)-1):\n",
    "                qc.cx(qubit, qubit+1)\n",
    "            qc.barrier()\n",
    "\n",
    "        return qc\n",
    "    \n",
    "    def run_circuit(self, data_point, params, circuit = 'real_amplitudes', shots = 100):\n",
    "        backend = AerSimulator(method = 'statevector')\n",
    "        if circuit == 'real_amplitudes':\n",
    "            qc = self.real_amplitudes(data_point, params)\n",
    "            \n",
    "        qc.measure_all()\n",
    "\n",
    "        tqc = transpile(qc, backend)\n",
    "\n",
    "        job = backend.run(tqc, shots=shots)\n",
    "        result = job.result()\n",
    "        counts = result.get_counts(qc)\n",
    "\n",
    "        return counts\n",
    "\n",
    "    \n",
    "    def data_decoding(self, output):\n",
    "        return int(output, 2) % self.num_classes\n",
    "    \n",
    "    def loss_function(self, updated_params):\n",
    "        shots = 100\n",
    "        predicted_probabilites = []\n",
    "        for x in self.X_train:\n",
    "            counts = self.run_circuit(x, updated_params, shots = shots)\n",
    "\n",
    "            count_classes = {x : 0 for x in range(self.num_classes)}\n",
    "            for output, count in counts.items():\n",
    "                class_num = self.data_decoding(output)\n",
    "                count_classes[class_num] += count / shots\n",
    "            \n",
    "\n",
    "            predicted_probabilites.append([count_classes[x] for x in range(self.num_classes)])        \n",
    "            \n",
    "        \n",
    "        logloss = log_loss(self.y_train, predicted_probabilites)\n",
    "\n",
    "        #print(f\"Parameters: {updated_params} loss: {logloss}\")\n",
    "        return logloss\n",
    "\n",
    "    def SPSA_optimize(self, maxiter = 50):\n",
    "        optimizer = SPSA(maxiter=maxiter)\n",
    "        # Optimize the parameters\n",
    "        optimized = optimizer.minimize(fun=self.loss_function, x0=self.initial_parameters)\n",
    "\n",
    "        print(\"Optimized Parameters:\", optimized.x)\n",
    "        print(\"Minimum Loss:\", optimized.fun)\n",
    "        self.optimized_params = optimized.x\n",
    "        self.min_loss = optimized.fun\n",
    "    \n",
    "    def save_parameters(self):\n",
    "        with open('optimized_parameters.txt', 'w') as file:\n",
    "            file.write(str(self.optimized_params))\n",
    "    \n",
    "\n",
    "    def gradient(self, params, epsilon = 0.2):\n",
    "        gradients = []\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            plus = [params[j] + epsilon if j == i else params[j] for j in range(len(params))]\n",
    "            minus = [params[j] - epsilon if j == i else params[j] for j in range(len(params))]\n",
    "\n",
    "            gradient = (self.loss_function(plus) - self.loss_function(minus)) / (2 * epsilon)\n",
    "            gradients.append(gradient)\n",
    "\n",
    "        return gradients\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, learning_rate = 0.1, maxiter = 50):\n",
    "        current_point = self.initial_parameters\n",
    "\n",
    "        for i in range(maxiter):\n",
    "            gradients = self.gradient(current_point)\n",
    "            current_point = [current_point[j] - learning_rate * gradients[j] for j in range(len(gradients))]\n",
    "            loss = self.loss_function(current_point)\n",
    "            print(f\"Iteration {i} Loss: {loss}\")\n",
    "        \n",
    "        print(\"Optimized Parameters:\", current_point, \"Loss:\", loss)\n",
    "        self.optimized_params = current_point\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def predict(self, data_point): #paramteres must be optimized before prediction\n",
    "        prediction_shots = 100000 # more shots as the circuit is only run once\n",
    "        counts = self.run_circuit(data_point, self.optimized_params, shots = prediction_shots)\n",
    "\n",
    "        predicted_probabilites = {x : 0 for x in range(self.num_classes)}\n",
    "            \n",
    "        # Decode each measurement outcome and aggregate probabilites for each class\n",
    "        for output, count in counts.items():\n",
    "            class_num = self.data_decoding(output)\n",
    "            predicted_probabilites[class_num] += count / prediction_shots\n",
    "        \n",
    "        \n",
    "        # Determine the predicted class by choosing the class with the highest probability\n",
    "        predicted_class = max(predicted_probabilites, key=predicted_probabilites.get)\n",
    "        \n",
    "        return predicted_class\n",
    "\n",
    "    def predict_dataset(self, X):\n",
    "        return [self.predict(data_point) for data_point in X]\n",
    "    \n",
    "    def performance(self, y_test, X_test):\n",
    "        return accuracy_score(y_test, self.predict_dataset(X_test))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.3, random_state=42) # 70% training \n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) # 15% validation, 15% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: [2.63145464 1.17877728 2.89736492 2.39084628 0.49586686 2.86500801\n",
      " 2.5911909  2.26949385 0.20248085 1.61492915 0.96489635 2.71151808] loss: 0.998770732615339\n",
      "Parameters: [2.23145464 1.57877728 2.49736492 1.99084628 0.09586686 3.26500801\n",
      " 2.1911909  2.66949385 0.60248085 1.21492915 1.36489635 3.11151808] loss: 1.2723476736941475\n",
      "Parameters: [2.23145464 1.17877728 2.49736492 2.39084628 0.49586686 2.86500801\n",
      " 2.1911909  2.66949385 0.20248085 1.21492915 1.36489635 3.11151808] loss: 1.1251839431082862\n",
      "Parameters: [2.63145464 1.57877728 2.89736492 1.99084628 0.09586686 3.26500801\n",
      " 2.5911909  2.26949385 0.60248085 1.61492915 0.96489635 2.71151808] loss: 0.9895173470599065\n",
      "Parameters: [2.23145464 1.57877728 2.49736492 2.39084628 0.09586686 2.86500801\n",
      " 2.5911909  2.26949385 0.60248085 1.21492915 1.36489635 3.11151808] loss: 1.1037163613558378\n",
      "Parameters: [2.63145464 1.17877728 2.89736492 1.99084628 0.49586686 3.26500801\n",
      " 2.1911909  2.66949385 0.20248085 1.61492915 0.96489635 2.71151808] loss: 1.1387850369273869\n",
      "Parameters: [2.63145464 1.57877728 2.89736492 2.39084628 0.09586686 3.26500801\n",
      " 2.1911909  2.26949385 0.60248085 1.61492915 0.96489635 2.71151808] loss: 1.017890645538628\n",
      "Parameters: [2.23145464 1.17877728 2.49736492 1.99084628 0.49586686 2.86500801\n",
      " 2.5911909  2.66949385 0.20248085 1.21492915 1.36489635 3.11151808] loss: 1.18224509303188\n",
      "Parameters: [2.23145464 1.57877728 2.89736492 1.99084628 0.09586686 3.26500801\n",
      " 2.1911909  2.66949385 0.20248085 1.61492915 0.96489635 3.11151808] loss: 1.2639310444935057\n",
      "Parameters: [2.63145464 1.17877728 2.49736492 2.39084628 0.49586686 2.86500801\n",
      " 2.5911909  2.26949385 0.60248085 1.21492915 1.36489635 2.71151808] loss: 1.022090048491031\n",
      "Parameters: [2.23145464 1.57877728 2.49736492 2.39084628 0.09586686 2.86500801\n",
      " 2.5911909  2.26949385 0.20248085 1.61492915 0.96489635 2.71151808] loss: 1.0314937122311387\n",
      "Parameters: [2.63145464 1.17877728 2.89736492 1.99084628 0.49586686 3.26500801\n",
      " 2.1911909  2.66949385 0.60248085 1.21492915 1.36489635 3.11151808] loss: 1.1735419384418213\n",
      "Parameters: [2.63145464 1.57877728 2.49736492 2.39084628 0.49586686 2.86500801\n",
      " 2.1911909  2.26949385 0.60248085 1.21492915 0.96489635 2.71151808] loss: 1.0426041341262038\n",
      "Parameters: [2.23145464 1.17877728 2.89736492 1.99084628 0.09586686 3.26500801\n",
      " 2.5911909  2.66949385 0.20248085 1.61492915 1.36489635 3.11151808] loss: 1.3046245058746275\n",
      "Parameters: [2.23145464 1.17877728 2.49736492 2.39084628 0.09586686 3.26500801\n",
      " 2.5911909  2.66949385 0.60248085 1.21492915 1.36489635 3.11151808] loss: 1.2022364527349032\n",
      "Parameters: [2.63145464 1.57877728 2.89736492 1.99084628 0.49586686 2.86500801\n",
      " 2.1911909  2.26949385 0.20248085 1.61492915 0.96489635 2.71151808] loss: 1.0237144684385224\n",
      "Parameters: [2.23145464 1.17877728 2.49736492 2.39084628 0.49586686 3.26500801\n",
      " 2.5911909  2.66949385 0.60248085 1.61492915 1.36489635 3.11151808] loss: 1.3158618950230458\n",
      "Parameters: [2.63145464 1.57877728 2.89736492 1.99084628 0.09586686 2.86500801\n",
      " 2.1911909  2.26949385 0.20248085 1.21492915 0.96489635 2.71151808] loss: 1.007334402033854\n",
      "Parameters: [2.23145464 1.17877728 2.89736492 1.99084628 0.49586686 3.26500801\n",
      " 2.5911909  2.26949385 0.60248085 1.21492915 0.96489635 3.11151808] loss: 1.1119565407141858\n",
      "Parameters: [2.63145464 1.57877728 2.49736492 2.39084628 0.09586686 2.86500801\n",
      " 2.1911909  2.66949385 0.20248085 1.61492915 1.36489635 2.71151808] loss: 1.1154109948900548\n",
      "Parameters: [2.23145464 1.57877728 2.89736492 1.99084628 0.49586686 2.86500801\n",
      " 2.1911909  2.66949385 0.20248085 1.21492915 0.96489635 3.11151808] loss: 1.1187839310740935\n",
      "Parameters: [2.63145464 1.17877728 2.49736492 2.39084628 0.09586686 3.26500801\n",
      " 2.5911909  2.26949385 0.60248085 1.61492915 1.36489635 2.71151808] loss: 1.0378575339333724\n",
      "Parameters: [2.23145464 1.57877728 2.49736492 2.39084628 0.09586686 3.26500801\n",
      " 2.1911909  2.66949385 0.20248085 1.21492915 1.36489635 2.71151808] loss: 1.1358247866035431\n",
      "Parameters: [2.63145464 1.17877728 2.89736492 1.99084628 0.49586686 2.86500801\n",
      " 2.5911909  2.26949385 0.60248085 1.61492915 0.96489635 3.11151808] loss: 1.0660133276622992\n",
      "Parameters: [2.23145464 1.57877728 2.89736492 2.39084628 0.49586686 2.86500801\n",
      " 2.5911909  2.26949385 0.60248085 1.21492915 0.96489635 2.71151808] loss: 0.9687311687226251\n",
      "Parameters: [2.63145464 1.17877728 2.49736492 1.99084628 0.09586686 3.26500801\n",
      " 2.1911909  2.66949385 0.20248085 1.61492915 1.36489635 3.11151808] loss: 1.305831753293146\n",
      "Parameters: [2.23145464 1.57877728 2.49736492 1.99084628 0.49586686 3.26500801\n",
      " 2.1911909  2.26949385 0.60248085 1.61492915 1.36489635 2.71151808] loss: 1.085417982818963\n",
      "Parameters: [2.63145464 1.17877728 2.89736492 2.39084628 0.09586686 2.86500801\n",
      " 2.5911909  2.66949385 0.20248085 1.21492915 0.96489635 3.11151808] loss: 1.0283380460940401\n",
      "Parameters: [2.63145464 1.57877728 2.49736492 2.39084628 0.09586686 2.86500801\n",
      " 2.1911909  2.66949385 0.20248085 1.61492915 1.36489635 2.71151808] loss: 1.1197806711474594\n",
      "Parameters: [2.23145464 1.17877728 2.89736492 1.99084628 0.49586686 3.26500801\n",
      " 2.5911909  2.26949385 0.60248085 1.21492915 0.96489635 3.11151808] loss: 1.110474900385721\n",
      "Parameters: [2.23145464 1.57877728 2.89736492 1.99084628 0.49586686 2.86500801\n",
      " 2.5911909  2.26949385 0.20248085 1.61492915 1.36489635 3.11151808] loss: 1.1816464238087905\n",
      "Parameters: [2.63145464 1.17877728 2.49736492 2.39084628 0.09586686 3.26500801\n",
      " 2.1911909  2.66949385 0.60248085 1.21492915 0.96489635 2.71151808] loss: 1.0972586614400694\n",
      "Parameters: [2.63145464 1.17877728 2.89736492 1.99084628 0.09586686 2.86500801\n",
      " 2.1911909  2.26949385 0.20248085 1.21492915 0.96489635 3.11151808] loss: 1.0491175064589155\n",
      "Parameters: [2.23145464 1.57877728 2.49736492 2.39084628 0.49586686 3.26500801\n",
      " 2.5911909  2.66949385 0.60248085 1.61492915 1.36489635 2.71151808] loss: 1.1878096639460756\n",
      "Parameters: [2.63145464 1.17877728 2.49736492 1.99084628 0.49586686 2.86500801\n",
      " 2.5911909  2.66949385 0.60248085 1.61492915 1.36489635 3.11151808] loss: 1.241373938284051\n",
      "Parameters: [2.23145464 1.57877728 2.89736492 2.39084628 0.09586686 3.26500801\n",
      " 2.1911909  2.26949385 0.20248085 1.21492915 0.96489635 2.71151808] loss: 1.0195084406111419\n",
      "Parameters: [2.23145464 1.57877728 2.49736492 1.99084628 0.09586686 2.86500801\n",
      " 2.1911909  2.66949385 0.20248085 1.61492915 0.96489635 3.11151808] loss: 1.1955804411583846\n",
      "Parameters: [2.63145464 1.17877728 2.89736492 2.39084628 0.49586686 3.26500801\n",
      " 2.5911909  2.26949385 0.60248085 1.21492915 1.36489635 2.71151808] loss: 0.9953089793278626\n",
      "Parameters: [2.63145464 1.57877728 2.89736492 2.39084628 0.49586686 3.26500801\n",
      " 2.1911909  2.66949385 0.20248085 1.61492915 0.96489635 2.71151808] loss: 1.13886510232426\n",
      "Parameters: [2.23145464 1.17877728 2.49736492 1.99084628 0.09586686 2.86500801\n",
      " 2.5911909  2.26949385 0.60248085 1.21492915 1.36489635 3.11151808] loss: 1.1322941778744349\n",
      "Parameters: [2.23145464 1.17877728 2.49736492 1.99084628 0.49586686 2.86500801\n",
      " 2.1911909  2.26949385 0.60248085 1.21492915 1.36489635 3.11151808] loss: 1.1257042225336822\n",
      "Parameters: [2.63145464 1.57877728 2.89736492 2.39084628 0.09586686 3.26500801\n",
      " 2.5911909  2.66949385 0.20248085 1.61492915 0.96489635 2.71151808] loss: 1.0730522453983942\n",
      "Parameters: [2.23145464 1.57877728 2.89736492 1.99084628 0.49586686 3.26500801\n",
      " 2.5911909  2.66949385 0.60248085 1.61492915 0.96489635 3.11151808] loss: 1.2469747234828514\n",
      "Parameters: [2.63145464 1.17877728 2.49736492 2.39084628 0.09586686 2.86500801\n",
      " 2.1911909  2.26949385 0.20248085 1.21492915 1.36489635 2.71151808] loss: 1.0518173422583537\n",
      "Parameters: [2.63145464 1.57877728 2.49736492 2.39084628 0.09586686 3.26500801\n",
      " 2.5911909  2.66949385 0.20248085 1.21492915 0.96489635 2.71151808] loss: 1.050765460376635\n",
      "Parameters: [2.23145464 1.17877728 2.89736492 1.99084628 0.49586686 2.86500801\n",
      " 2.1911909  2.26949385 0.60248085 1.61492915 1.36489635 3.11151808] loss: 1.0996590892345293\n",
      "Parameters: [2.23145464 1.17877728 2.49736492 2.39084628 0.09586686 3.26500801\n",
      " 2.5911909  2.26949385 0.60248085 1.21492915 1.36489635 2.71151808] loss: 1.0304221678648062\n",
      "Parameters: [2.63145464 1.57877728 2.89736492 1.99084628 0.49586686 2.86500801\n",
      " 2.1911909  2.66949385 0.20248085 1.61492915 0.96489635 3.11151808] loss: 1.1702848214599852\n",
      "Parameters: [2.63145464 1.17877728 2.89736492 1.99084628 0.09586686 3.26500801\n",
      " 2.5911909  2.26949385 0.20248085 1.61492915 1.36489635 3.11151808] loss: 1.1856296630215133\n",
      "Parameters: [2.23145464 1.57877728 2.49736492 2.39084628 0.49586686 2.86500801\n",
      " 2.1911909  2.66949385 0.60248085 1.21492915 0.96489635 2.71151808] loss: 1.0618802352556969\n",
      "Parameters: [2.63145464 1.57877728 2.89736492 1.99084628 0.09586686 2.86500801\n",
      " 2.5911909  2.26949385 0.60248085 1.21492915 0.96489635 3.11151808] loss: 1.0216531646159388\n",
      "Parameters: [2.23145464 1.17877728 2.49736492 2.39084628 0.49586686 3.26500801\n",
      " 2.1911909  2.66949385 0.20248085 1.61492915 1.36489635 2.71151808] loss: 1.1865270099970282\n",
      "Parameters: [ 3.3554802   2.30280285  3.62139048  1.63977531 -0.6281587   2.51393704\n",
      "  3.31521646  1.91842287  1.32650641  0.86385818  0.24087078  3.83554365] loss: 1.0546609400788804\n",
      "Parameters: [ 2.98252561  1.92984825  3.24843589  1.26682071 -0.25520411  2.14098245\n",
      "  2.94226187  1.54546828  0.95355182  0.49090359  0.61382538  3.46258905] loss: 0.9384805215112614\n",
      "Parameters: [ 2.9807531   1.57008587  2.8886735   1.2650482  -0.2534316   2.13920994\n",
      "  2.94048936  1.54369577  0.95177931  0.1311412   0.97358776  3.46081654] loss: 1.032389212923863\n",
      "Parameters: [2.62276322 1.92807574 3.24666338 0.90705833 0.10455828 1.78122006\n",
      " 2.58249948 1.18570589 0.59378943 0.48913108 0.61559789 3.10282667] loss: 0.9164798697151569\n",
      "Parameters: [2.67659389 1.87424507 3.54057053 0.61315117 0.05072761 1.83505073\n",
      " 2.63633015 0.89179874 0.6476201  0.43530041 0.66942856 2.80891951] loss: 0.8289342970901448\n",
      "Parameters: [2.32885607 2.2219829  3.19283271 0.960889   0.39846543 1.48731291\n",
      " 2.28859233 1.23953657 0.29988228 0.78303823 0.32169073 3.15665734] loss: 1.0343890921155432\n",
      "Parameters: [ 3.13162428  1.41921469  3.65561258  0.49810913 -0.40430277  1.95009277\n",
      "  2.7513722   0.43676836  1.10265049  0.32025836  1.12445894  2.69387747] loss: 0.7914231053452085\n",
      "Parameters: [ 2.79163594  1.75920303  3.99560091  0.15812079 -0.06431444  2.29008111\n",
      "  3.09136053  0.7767567   0.76266215 -0.01972997  0.7844706   2.35388913] loss: 0.830327481058991\n",
      "Parameters: [ 3.20622832  1.67839557  3.58100853  0.57271317 -0.47890682  1.87548873\n",
      "  3.01055308  0.36216432  1.17725453  0.39486241  0.86527806  2.76848151] loss: 0.7679808570120312\n",
      "Parameters: [ 2.8724434   1.34461065  3.91479346  0.23892825 -0.14512189  2.20927365\n",
      "  2.67676815  0.69594924  0.84346961  0.06107748  1.19906298  2.43469659] loss: 0.8684217477011771\n",
      "Parameters: [ 3.38675306  1.85892031  3.4004838   0.75323791 -0.65943155  1.69496399\n",
      "  3.19107781  0.18163958  1.02915087  0.57538714  0.68475332  2.94900625] loss: 0.8167184635255684\n",
      "Parameters: [ 3.05812466  1.53029191  3.72911219  0.42460951 -0.33080316  2.02359239\n",
      "  2.86244942  0.51026798  1.35777927  0.24675875  1.01338172  2.62037785] loss: 0.7767635660171536\n",
      "Parameters: [ 2.99290182  1.78929512  3.79433503  0.68361272 -0.26558032  1.76458918\n",
      "  2.79722658  0.25126477  1.09877605  0.18153591  1.07860456  2.87938107] loss: 0.7762788886168412\n",
      "Parameters: [ 3.31712787  1.46506908  3.47010898  0.35938668 -0.58980637  2.08881523\n",
      "  3.12145263  0.57549081  1.4230021   0.50576196  0.75437851  2.55515502] loss: 0.8006099141346578\n",
      "Parameters: [ 2.95641716  1.50538791  3.51042782  0.39970551 -0.22909566  2.04849639\n",
      "  3.08113379  0.2147801   1.06229139  0.14505125  1.11508922  2.59547385] loss: 0.7485193440663747\n",
      "Parameters: [ 3.27680903  1.82577979  3.83081969  0.72009739 -0.54948753  1.72810452\n",
      "  2.76074192  0.53517198  1.38268327  0.46544312  0.79469734  2.91586573] loss: 0.8093951293085189\n",
      "Parameters: [ 2.86753795  1.4165087   3.73854914  0.62782684 -0.45721698  1.82037507\n",
      "  3.170013    0.44290143  1.29041272  0.05617204  0.88696789  2.50659465] loss: 0.8296916285504773\n",
      "Parameters: [ 3.18453848  1.73350924  3.42154861  0.31082631 -0.14021645  2.1373756\n",
      "  2.85301247  0.1259009   0.97341218  0.37317257  1.20396843  2.82359518] loss: 0.7161886396744781\n",
      "Parameters: [ 3.34321393  1.57822106  3.57683679  0.15215086  0.018459    1.98208742\n",
      "  2.69433702 -0.03277455  1.12870036  0.53184802  1.36264387  2.668307  ] loss: 0.7951602976527471\n",
      "Parameters: [ 3.02925031  1.89218468  3.26287316  0.46611448 -0.29550463  2.29605105\n",
      "  3.00830064  0.28118907  0.81473674  0.2178844   1.04868025  2.98227063] loss: 0.7128790540963081\n",
      "Parameters: [ 2.91991003  1.69030841  3.15353289  0.57545475 -0.4048449   2.40539132\n",
      "  3.11764092  0.39052935  1.01661301  0.10854412  0.93933998  3.0916109 ] loss: 0.7174122038923038\n",
      "Parameters: [ 3.23112659  2.00152496  3.46474944  0.2642382  -0.09362835  2.09417477\n",
      "  2.80642436  0.07931279  0.70539646  0.41976067  1.25055653  2.78039435] loss: 0.7342192611918443\n",
      "Parameters: [ 3.20822354  1.66991119  3.13313567  0.59585197 -0.1165314   2.42578854\n",
      "  3.13803813  0.41092656  0.72829951  0.08814691  1.22765348  2.8032974 ] loss: 0.7537086697589828\n",
      "Parameters: [ 2.89951282  1.97862191  3.44184639  0.28714125 -0.42524212  2.11707782\n",
      "  2.82932741  0.10221584  1.03701023  0.39685763  0.91894276  3.11200812] loss: 0.6812422924647845\n",
      "Parameters: [ 2.81098508  1.76074097  3.22396546  0.50502219 -0.20736118  2.02855007\n",
      "  2.74079967  0.32009678  0.8191293   0.17897669  1.13682369  2.89412718] loss: 0.7540107082731913\n",
      "Parameters: [ 3.11739375  2.06714965  3.53037414  0.19861351 -0.51376986  2.33495875\n",
      "  3.04720835  0.0136881   1.12553797  0.48538537  0.83041502  3.20053586] loss: 0.6845177387111895\n",
      "Parameters: [ 3.19919456  2.14895046  3.30789399  0.42109366 -0.29128971  2.1124786\n",
      "  3.12900916  0.23616825  0.90305783  0.56718618  1.05289516  2.97805571] loss: 0.7172032198109592\n",
      "Parameters: [ 2.89491361  1.8446695   3.61217495  0.1168127  -0.59557067  2.41675956\n",
      "  2.8247282  -0.06811271  1.20733878  0.26290522  0.74861421  3.28233667] loss: 0.6927484270812992\n",
      "Parameters: [ 3.17003646  2.11979236  3.33705209  0.39193555 -0.6227518   2.44394069\n",
      "  2.79754707  0.20701014  1.23451992  0.53802808  1.02373706  3.00721381] loss: 0.7485356603510093\n",
      "Parameters: [ 2.86773247  1.81748837  3.63935608  0.08963157 -0.32044781  2.14163671\n",
      "  3.09985105 -0.09529384  0.93221593  0.23572409  0.72143308  3.3095178 ] loss: 0.6442696406634218\n",
      "Parameters: [ 2.75237186  2.00258636  3.45425808  0.27472956 -0.2050872   2.3267347\n",
      "  3.21521167 -0.21065446  1.11731392  0.12036347  0.90653107  3.42487842] loss: 0.6974379271367118\n",
      "Parameters: [ 3.05283047  1.70212776  3.75471669 -0.02572905 -0.50554581  2.02627609\n",
      "  2.91475306  0.08980415  0.81685531  0.42082208  0.60607246  3.12441981] loss: 0.6494004260146867\n",
      "Parameters: [ 2.80520801  1.65102116  3.50709423  0.22189341 -0.25792335  1.9751695\n",
      "  2.86364647 -0.15781831  1.06447778  0.47192868  0.55496586  3.07331321] loss: 0.6636573646238509\n",
      "Parameters: [ 3.10393706  1.94975022  3.80582329 -0.07683564 -0.5566524   2.27389855\n",
      "  3.16237552  0.14091075  0.76574872  0.17319962  0.85369492  3.37204227] loss: 0.691896685079998\n",
      "Parameters: [ 2.77633203  1.62214519  3.77532046 -0.04633282 -0.52614958  2.24339573\n",
      "  2.83477049 -0.18669428  1.09335375  0.50080465  0.52608989  3.04443724] loss: 0.6837653377651345\n",
      "Parameters: [ 3.07343424  1.91924739  3.47821826  0.25076939 -0.22904737  1.94629352\n",
      "  3.1318727   0.11040792  0.79625154  0.20370244  0.8231921   3.34153944] loss: 0.6543566800942452\n",
      "Parameters: [ 3.10275915  1.65300529  3.74446036  0.28009429 -0.19972247  2.21253563\n",
      "  3.16119761 -0.15583418  1.06249365  0.46994455  0.55694999  3.07529734] loss: 0.6505672055998385\n",
      "Parameters: [ 2.80719213  1.9485723   3.44889335 -0.01547272 -0.49528948  1.91696861\n",
      "  2.86563059  0.13973283  0.76692663  0.17437754  0.852517    3.37086435] loss: 0.6699645680955467\n",
      "Parameters: [ 2.82726344  1.928501    3.46896466  0.00459859 -0.18110407  1.93703992\n",
      "  2.8857019   0.11966152  1.08111204  0.19444884  0.8324457   3.05667895] loss: 0.6586952443751465\n",
      "Parameters: [ 3.12137754  1.6343869   3.76307875  0.29871268 -0.47521817  2.23115402\n",
      "  3.179816   -0.17445257  0.78699794  0.48856294  0.5383316   3.35079305] loss: 0.6420801628226787\n",
      "Parameters: [ 2.84412274  1.91164169  3.48582396  0.31419333 -0.49069882  2.24663466\n",
      "  3.19529664 -0.18993322  0.7715173   0.50404358  0.52285096  3.36627369] loss: 0.6527796342928379\n",
      "Parameters: [ 3.13685818  1.61890625  3.7785594   0.02145789 -0.19796338  1.95389922\n",
      "  2.9025612   0.10280222  1.06425274  0.21130815  0.8155864   3.07353825] loss: 0.6548079524117045\n",
      "Parameters: [ 3.13427401  1.91291453  3.77597523  0.02404206 -0.20054755  2.2479075\n",
      "  2.90514537  0.10021805  0.77024446  0.50531643  0.52157812  3.36754653] loss: 0.6179598402341141\n",
      "Parameters: [ 2.8428499   1.62149042  3.48455112  0.31546617 -0.49197166  1.95648339\n",
      "  3.19656948 -0.19120606  1.06166857  0.21389232  0.81300223  3.07612242] loss: 0.6709631805341607\n",
      "Parameters: [ 3.18293418  1.6714006   3.53446129  0.265556   -0.15188738  2.29656767\n",
      "  3.14665931 -0.14129588  1.01175839  0.26380249  0.76309205  3.4162067 ] loss: 0.6331514544033647\n",
      "Parameters: [ 2.89276007  1.9615747   3.8246354  -0.02461811 -0.44206148  2.00639357\n",
      "  2.8564852   0.14887822  0.72158429  0.5539766   0.47291795  3.12603259] loss: 0.6614385636147239\n",
      "Parameters: [ 2.91910492  1.93522986  3.50931038  0.29070691 -0.41571664  2.03273841\n",
      "  2.88283004  0.12253338  0.74792913  0.23865158  0.49926279  3.44135761] loss: 0.6466782400740186\n",
      "Parameters: [ 3.20808509e+00  1.64624969e+00  3.79829056e+00  1.72673085e-03\n",
      " -1.26736467e-01  2.32171858e+00  3.17181022e+00 -1.66446795e-01\n",
      "  1.03690930e+00  5.27631755e-01  7.88242963e-01  3.15237744e+00] loss: 0.644122537079495\n",
      "Parameters: [ 2.92195534  1.93237943  3.51216081  0.28785648 -0.12502851  2.32342654\n",
      "  3.17351818  0.11968295  0.75077956  0.52933971  0.78995092  3.43850718] loss: 0.6335339058177664\n",
      "Parameters: [ 3.20979305e+00  1.64454173e+00  3.79999851e+00  1.87730396e-05\n",
      " -4.12866214e-01  2.03558884e+00  2.88568047e+00 -1.68154753e-01\n",
      "  1.03861726e+00  2.41502008e-01  5.02113216e-01  3.15066948e+00] loss: 0.6570338850931198\n",
      "Parameters: [ 2.90195323  1.95238155  3.4921587   0.02111597 -0.1050264   2.05668603\n",
      "  2.90677767  0.13968506  0.73077744  0.2625992   0.80995303  3.4585093 ] loss: 0.6517430520457652\n",
      "Parameters: [ 3.18869585  1.66563893  3.77890132  0.30785859 -0.39176902  2.34342865\n",
      "  3.19352029 -0.14705756  1.01752007  0.54934182  0.52321041  3.17176667] loss: 0.6528002239038152\n",
      "Parameters: [ 2.90157175  1.66707171  3.49177722  0.02073449 -0.10464492  2.34199587\n",
      "  2.90639619  0.14006654  0.73039597  0.26221773  0.5246432   3.45889077] loss: 0.6200196810888461\n",
      "Parameters: [ 3.18726306  1.95276302  3.77746853  0.3064258  -0.39033623  2.05630456\n",
      "  3.1920875  -0.14562477  1.01608728  0.54790904  0.81033451  3.17319946] loss: 0.7052782494056824\n",
      "Parameters: [ 2.83023975  1.59573971  3.42044521  0.23408303 -0.31799346  2.41332787\n",
      "  2.83506418  0.21139855  0.94374451  0.19088572  0.45331119  3.24554223] loss: 0.633212891965653\n",
      "Parameters: [ 3.11492029  1.88042025  3.70512576 -0.05059752 -0.03331291  2.12864733\n",
      "  3.11974473 -0.073282    0.65906396  0.47556627  0.73799174  3.53022278] loss: 0.6400671261514532\n",
      "Parameters: [ 2.82505168  1.87425909  3.41525715  0.2392711  -0.03947407  2.13480849\n",
      "  2.82987612 -0.06712084  0.94893257  0.18569765  0.44812313  3.24035417] loss: 0.6399708366706902\n",
      "Parameters: [ 3.10875913  1.59055164  3.6989646  -0.04443636 -0.32318152  2.41851594\n",
      "  3.11358357  0.21658661  0.66522512  0.46940511  0.73183058  3.52406162] loss: 0.6203679256041902\n",
      "Parameters: [ 2.8414766   1.57506474  3.43168206 -0.05992325 -0.33866842  2.43400284\n",
      "  3.12907047 -0.05069592  0.64973822  0.20212257  0.74731748  3.53954852] loss: 0.625845627158552\n",
      "Parameters: [ 3.12424603  1.85783418  3.7144515   0.22284618 -0.05589899  2.1512334\n",
      "  2.84630104  0.23207351  0.93250765  0.484892    0.46454804  3.25677909] loss: 0.6215183289078995\n",
      "Parameters: [ 3.12725823  1.57898222  3.7174637   0.22585838 -0.33475094  2.43008535\n",
      "  3.12515299 -0.04677844  0.6536557   0.20604005  0.46153584  3.53563104] loss: 0.6099969044772892\n",
      "Parameters: [ 2.84539408  1.86084638  3.43559954 -0.05600577 -0.05288679  2.1482212\n",
      "  2.84328883  0.23508571  0.93551985  0.4879042   0.7434      3.25376689] loss: 0.6322401190296114\n",
      "Parameters: [ 3.14434986  1.84288009  3.45356583  0.24295001 -0.07085307  2.44717698\n",
      "  3.14224462 -0.06387007  0.91755357  0.46993792  0.72543371  3.27173317] loss: 0.6160985534553939\n",
      "Parameters: [ 2.86336037  1.5618906   3.73455533 -0.03803949 -0.35184257  2.16618749\n",
      "  2.86125512  0.21711943  0.63656407  0.18894842  0.44444421  3.55272267] loss: 0.6471158924879306\n",
      "Parameters: [ 2.88785288  1.58638311  3.71006281 -0.01354698 -0.04720651  2.19068\n",
      "  3.16589118 -0.08751663  0.66105658  0.49358448  0.46893673  3.24808661] loss: 0.6123810138996404\n",
      "Parameters: [ 3.16799642  1.86652665  3.42991927  0.26659657 -0.32735006  2.47082355\n",
      "  2.88574763  0.19262691  0.94120013  0.21344093  0.74908027  3.52823015] loss: 0.6631414842357798\n",
      "Parameters: [ 2.84945686  1.82731165  3.74845883  0.22738157 -0.0088105   2.43160854\n",
      "  2.92496264 -0.12591265  0.90198513  0.53198049  0.70986527  3.48901515] loss: 0.646927693869784\n",
      "Parameters: [ 3.12878142  1.54798709  3.46913427 -0.05194299 -0.28813505  2.15228399\n",
      "  3.20428719  0.15341191  0.62266057  0.25265594  0.43054071  3.20969059] loss: 0.6051218849166926\n",
      "Parameters: [ 2.88135271  1.79541581  3.71656298 -0.08304521 -0.04070634  2.12118176\n",
      "  3.23538941  0.18451413  0.87008928  0.50008465  0.39943849  3.45711931] loss: 0.6256561800556258\n",
      "Parameters: [ 3.15988364  1.51688487  3.43803205  0.19548572 -0.31923728  2.3997127\n",
      "  2.95685848 -0.0940168   0.59155835  0.22155371  0.67796943  3.17858837] loss: 0.6216707307963463\n",
      "Parameters: [ 3.16245958  1.51430893  3.71321733 -0.07969956 -0.04405199  2.40228864\n",
      "  2.95428254 -0.09659274  0.86674363  0.496739    0.40278414  3.45377366] loss: 0.5960304105227131\n",
      "Parameters: [ 2.88469836  1.79207015  3.43545611  0.19806166 -0.32181322  2.12452742\n",
      "  3.23204376  0.18116848  0.58898241  0.21897777  0.68054536  3.17601243] loss: 0.6392935541666993\n",
      "Parameters: [ 3.19378809  1.75999451  3.46753175  0.16598602 -0.28973757  2.15660306\n",
      "  3.19996812  0.14909284  0.89807214  0.25105342  0.37145564  3.48510216] loss: 0.6186746022785421\n",
      "Parameters: [ 2.916774    1.48298043  3.74454584 -0.11102807 -0.01272349  2.43361714\n",
      "  2.92295403 -0.12792125  0.62105805  0.5280675   0.64846972  3.20808808] loss: 0.6308534398892294\n",
      "Parameters: [ 3.20223107  1.49214921  3.73537705  0.174429   -0.02189227  2.14816008\n",
      "  2.93212282 -0.11875246  0.63022684  0.24261044  0.63930093  3.49354514] loss: 0.6411360361691963\n",
      "Parameters: [ 2.92594279  1.76843749  3.45908877 -0.10185928 -0.29818055  2.42444836\n",
      "  3.2084111   0.15753582  0.90651512  0.51889872  0.36301265  3.21725686] loss: 0.5969612125527823\n",
      "Parameters: [ 3.17035073  1.52402955  3.70349671 -0.13303403 -0.3293553   2.45562311\n",
      "  3.23958585 -0.08687212  0.66210718  0.27449078  0.33183791  3.18608212] loss: 0.579824596406984\n",
      "Parameters: [ 2.89476804  1.79961224  3.42791402  0.14254866 -0.05377261  2.18004042\n",
      "  2.96400316  0.18871057  0.93768987  0.55007347  0.60742059  3.4616648 ] loss: 0.6185941943199114\n",
      "Parameters: [ 3.1973286   1.77194794  3.45557833  0.11488436 -0.08143691  2.20770472\n",
      "  3.26656371  0.16104627  0.91002557  0.52240916  0.57975629  3.4340005 ] loss: 0.6121077307014458\n",
      "Parameters: [ 2.92243234  1.49705168  3.73047458 -0.16001189 -0.35633317  2.48260097\n",
      "  2.99166746 -0.11384999  0.63512931  0.24751291  0.30486004  3.15910425] loss: 0.6099751540929612\n",
      "Parameters: [ 3.19551012  1.49590145  3.45739681  0.11306588 -0.08325539  2.2095232\n",
      "  3.26474523 -0.11500022  0.63397908  0.52059069  0.57793781  3.43218202] loss: 0.6020286405830451\n",
      "Parameters: [ 2.92128211  1.77012946  3.73162482 -0.16116213 -0.3574834   2.48375121\n",
      "  2.99051722  0.15922779  0.90820709  0.24636267  0.3037098   3.15795401] loss: 0.6135368399047234\n",
      "Parameters: [ 3.20309868  1.48831288  3.44980824 -0.15292262 -0.34924389  2.4755117\n",
      "  2.99875674 -0.12258879  0.62639051  0.25460219  0.58552638  3.43977059] loss: 0.5991587450314866\n",
      "Parameters: [ 2.92952162  1.76188994  3.7233853   0.12065444 -0.07566683  2.20193464\n",
      "  3.2723338   0.15098827  0.89996757  0.52817925  0.31194932  3.16619353] loss: 0.6168261804074934\n",
      "Parameters: [ 2.94184614  1.47662286  3.43811821  0.10832992 -0.36093392  2.48720173\n",
      "  3.26000928  0.13866375  0.88764305  0.51585473  0.32427384  3.45146062] loss: 0.5766720061797549\n",
      "Parameters: [ 3.21478871  1.74956542  3.71106078 -0.16461265 -0.08799135  2.21425916\n",
      "  2.98706671 -0.13427882  0.61470048  0.24291216  0.59721641  3.17851805] loss: 0.630956563521397\n",
      "Parameters: [ 3.17800851  1.44046147  3.40195683  0.14449131 -0.3970953   2.25103936\n",
      "  3.02384691  0.17482514  0.65148068  0.27969236  0.28811245  3.21529825] loss: 0.5914650470875364\n",
      "Parameters: [ 2.90568476  1.71278522  3.67428058 -0.12783245 -0.12477155  2.52336311\n",
      "  3.29617066 -0.09749862  0.92380444  0.55201611  0.56043621  3.487622  ] loss: 0.6107097638504166\n",
      "Parameters: [ 3.19049129  1.42797869  3.66119395  0.15697409 -0.13785818  2.23855658\n",
      "  3.01136413 -0.08441199  0.9107178   0.53892948  0.27562967  3.47453537] loss: 0.5933312041532418\n",
      "Parameters: [ 2.91877139  1.69969859  3.38947405 -0.11474582 -0.40957808  2.51027648\n",
      "  3.28308403  0.18730792  0.6389979   0.26720958  0.54734958  3.20281547] loss: 0.604463838731394\n",
      "Parameters: [ 2.92638085  1.4209588   3.66821384 -0.10713636 -0.13083829  2.50266702\n",
      "  3.27547457 -0.09143188  0.64660736  0.54594937  0.26860979  3.48155526] loss: 0.5673766426081546\n",
      "Parameters: [ 3.19751118  1.69208913  3.39708351  0.16399397 -0.40196863  2.23153669\n",
      "  3.00434424  0.17969846  0.91773769  0.27481904  0.53974012  3.21042493] loss: 0.6161994719299011\n",
      "Parameters: [ 2.89493365  1.66006602  3.42910662 -0.13858355 -0.0993911   2.53411422\n",
      "  3.03636735  0.14767535  0.88571458  0.57739656  0.50771701  3.51300245] loss: 0.5885479694724934\n",
      "Parameters: [ 3.16548807  1.38951161  3.69966103  0.13197086 -0.36994551  2.2635598\n",
      "  3.30692176 -0.12287907  0.61516017  0.30684215  0.23716259  3.24244804] loss: 0.5901513916759075\n",
      "Parameters: [ 2.89418378  1.66081589  3.69834831 -0.13933342 -0.36863279  2.26487253\n",
      "  3.30560904 -0.12156634  0.61647289  0.57814643  0.50846688  3.51375232] loss: 0.6356940525870098\n",
      "Parameters: [ 3.16417534  1.39082433  3.42835675  0.13065814 -0.09864123  2.53486409\n",
      "  3.03561748  0.14842522  0.88646445  0.30815487  0.23847532  3.24376077] loss: 0.561372730529865\n",
      "Parameters: [ 2.94177015  1.61322953  3.38132073  0.17769415 -0.32104642  2.5819001\n",
      "  2.98858146  0.19546123  0.93350047  0.53056007  0.1914393   3.46616596] loss: 0.573664977381425\n",
      "Parameters: [ 3.21121136  1.34378832  3.65076194 -0.09174706 -0.05160521  2.31245889\n",
      "  3.25802267 -0.07397998  0.66405926  0.26111886  0.46088051  3.19672475] loss: 0.5896460402783316\n",
      "Parameters: [ 3.06642003  1.48857964  3.50597062  0.05304426 -0.19639653  2.45725021\n",
      "  3.11323135  0.07081134  0.80885058  0.40591018  0.31608919  3.34151607] loss: 0.5646078153903674\n",
      "Optimized Parameters: [ 3.06642003  1.48857964  3.50597062  0.05304426 -0.19639653  2.45725021\n",
      "  3.11323135  0.07081134  0.80885058  0.40591018  0.31608919  3.34151607]\n",
      "Minimum Loss: 0.5646078153903674\n"
     ]
    }
   ],
   "source": [
    "spsa = QuantumMachineLearning(X_train, y_train)\n",
    "spsa.SPSA_optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9130434782608695"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spsa.save_parameters()\n",
    "spsa.performance(y_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Loss: 1.0445334984632195\n",
      "Iteration 1 Loss: 0.9925891454689159\n",
      "Iteration 2 Loss: 0.945551791906504\n",
      "Iteration 3 Loss: 0.9323700530917735\n",
      "Iteration 4 Loss: 0.9023047300957192\n",
      "Iteration 5 Loss: 0.8846784378897742\n",
      "Iteration 6 Loss: 0.8614149246932473\n",
      "Iteration 7 Loss: 0.8483816342153252\n",
      "Iteration 8 Loss: 0.8321334545476545\n",
      "Iteration 9 Loss: 0.8282281750553359\n",
      "Iteration 10 Loss: 0.8132293218687842\n",
      "Iteration 11 Loss: 0.8111309373200807\n",
      "Iteration 12 Loss: 0.7930501685926176\n",
      "Iteration 13 Loss: 0.7911531626226448\n",
      "Iteration 14 Loss: 0.7845376799414653\n",
      "Iteration 15 Loss: 0.7691073324008648\n",
      "Iteration 16 Loss: 0.7663872384979388\n",
      "Iteration 17 Loss: 0.7641348824037416\n",
      "Iteration 18 Loss: 0.7532199301132393\n",
      "Iteration 19 Loss: 0.7575587348507932\n",
      "Iteration 20 Loss: 0.746092949110698\n",
      "Iteration 21 Loss: 0.7450456371640717\n",
      "Iteration 22 Loss: 0.743193398679631\n",
      "Iteration 23 Loss: 0.7408188844649771\n",
      "Iteration 24 Loss: 0.7398960354584693\n",
      "Iteration 25 Loss: 0.7417773557386574\n",
      "Iteration 26 Loss: 0.7329919754689491\n",
      "Iteration 27 Loss: 0.7337459562551523\n",
      "Iteration 28 Loss: 0.726805983803974\n",
      "Iteration 29 Loss: 0.7283841255974632\n",
      "Iteration 30 Loss: 0.7239025684114357\n",
      "Iteration 31 Loss: 0.7232917890313397\n",
      "Iteration 32 Loss: 0.7258023779485911\n",
      "Iteration 33 Loss: 0.7232219395738136\n",
      "Iteration 34 Loss: 0.721070368815377\n",
      "Iteration 35 Loss: 0.7116390628365856\n",
      "Iteration 36 Loss: 0.7163860435001737\n",
      "Iteration 37 Loss: 0.7108782520177427\n",
      "Iteration 38 Loss: 0.7062530368528261\n",
      "Iteration 39 Loss: 0.7119519369711538\n",
      "Iteration 40 Loss: 0.7037995746638459\n",
      "Iteration 41 Loss: 0.7012991424550604\n",
      "Iteration 42 Loss: 0.6977088754811475\n",
      "Iteration 43 Loss: 0.6901172045881807\n",
      "Iteration 44 Loss: 0.6900600611910107\n",
      "Iteration 45 Loss: 0.6785035771334059\n",
      "Iteration 46 Loss: 0.6808032964839913\n",
      "Iteration 47 Loss: 0.6731865195929366\n",
      "Iteration 48 Loss: 0.6686762657452039\n",
      "Iteration 49 Loss: 0.6604100538058093\n",
      "Iteration 50 Loss: 0.6583416759970953\n",
      "Iteration 51 Loss: 0.6529043471529097\n",
      "Iteration 52 Loss: 0.6446628669121262\n",
      "Iteration 53 Loss: 0.6413760485026734\n",
      "Iteration 54 Loss: 0.6324836291390521\n",
      "Iteration 55 Loss: 0.6259309199269538\n",
      "Iteration 56 Loss: 0.6146929147666454\n",
      "Iteration 57 Loss: 0.6175091343263457\n",
      "Iteration 58 Loss: 0.60329774671942\n",
      "Iteration 59 Loss: 0.589943683259078\n",
      "Iteration 60 Loss: 0.5829200835072653\n",
      "Iteration 61 Loss: 0.574121420291677\n",
      "Iteration 62 Loss: 0.5679843955106115\n",
      "Iteration 63 Loss: 0.5630845678502079\n",
      "Iteration 64 Loss: 0.5509880679962639\n",
      "Iteration 65 Loss: 0.5475723386003238\n",
      "Iteration 66 Loss: 0.541302865416626\n",
      "Iteration 67 Loss: 0.5390614714584016\n",
      "Iteration 68 Loss: 0.5292621231638133\n",
      "Iteration 69 Loss: 0.5230374489145122\n",
      "Iteration 70 Loss: 0.527549866577125\n",
      "Iteration 71 Loss: 0.518487872149551\n",
      "Iteration 72 Loss: 0.5229828787399711\n",
      "Iteration 73 Loss: 0.5139294242636102\n",
      "Iteration 74 Loss: 0.5133292837338826\n",
      "Iteration 75 Loss: 0.5057628441586279\n",
      "Iteration 76 Loss: 0.5069358173926944\n",
      "Iteration 77 Loss: 0.5004582662591289\n",
      "Iteration 78 Loss: 0.5020736735497074\n",
      "Iteration 79 Loss: 0.49836349444871814\n",
      "Iteration 80 Loss: 0.49681301031936254\n",
      "Iteration 81 Loss: 0.49853312499346425\n",
      "Iteration 82 Loss: 0.4955298020494334\n",
      "Iteration 83 Loss: 0.49089267143727033\n",
      "Iteration 84 Loss: 0.48629104127413725\n",
      "Iteration 85 Loss: 0.49309705981722673\n",
      "Iteration 86 Loss: 0.48816574190300255\n",
      "Iteration 87 Loss: 0.48281863620851884\n",
      "Iteration 88 Loss: 0.49265609686171435\n",
      "Iteration 89 Loss: 0.488858730153555\n",
      "Iteration 90 Loss: 0.48520091554742223\n",
      "Iteration 91 Loss: 0.48623672648316124\n",
      "Iteration 92 Loss: 0.48293257655251504\n",
      "Iteration 93 Loss: 0.4862771775728329\n",
      "Iteration 94 Loss: 0.4832936698108157\n",
      "Iteration 95 Loss: 0.48001641377173\n",
      "Iteration 96 Loss: 0.48287637109107223\n",
      "Iteration 97 Loss: 0.4778000496545437\n",
      "Iteration 98 Loss: 0.48411955921386296\n",
      "Iteration 99 Loss: 0.47936933626751804\n",
      "Optimized Parameters: [np.float64(2.654383781895941), np.float64(2.095688838346267), np.float64(3.1175233738196564), np.float64(2.6649706203326966), np.float64(0.459101350315915), np.float64(3.4675210429581105), np.float64(3.3942527841319934), np.float64(2.8430543683557623), np.float64(0.30875696141642317), np.float64(2.3697794940301575), np.float64(0.21286345679817986), np.float64(0.5941253571749291)] Loss: 0.47936933626751804\n"
     ]
    }
   ],
   "source": [
    "ours = QuantumMachineLearning(X_train, y_train)\n",
    "ours.gradient_descent(learning_rate=0.5, maxiter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8260869565217391"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ours.save_parameters()\n",
    "ours.performance(y_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
